"""
å•ç­”å¼•æ“ - æ•´åˆ RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ)
"""
from typing import List, Dict, Any
from groq import Groq
from config import Config


class QAEngine:
    """å•ç­”å¼•æ“"""
    
    def __init__(self, vector_store):
        self.vector_store = vector_store
        self.client = Groq(api_key=Config.GROQ_API_KEY)
        self.model = Config.LLM_MODEL
    
    def generate_answer(self, question: str, context_docs: List[Dict[str, Any]]) -> str:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        
        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            context_docs: ç›¸é—œæ–‡ä»¶åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        # çµ„åˆä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"[åƒè€ƒè³‡æ–™ {i+1}]\n{doc['content']}"
            for i, doc in enumerate(context_docs)
        ])
        
        # å»ºç«‹æç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è¾²æ¥­é¡§å•ï¼Œè«‹æ ¹æ“šä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

åƒè€ƒè³‡æ–™ï¼š
{context}

ä½¿ç”¨è€…å•é¡Œï¼š{question}

è«‹æ ¹æ“šä¸Šè¿°åƒè€ƒè³‡æ–™æä¾›å°ˆæ¥­ã€æº–ç¢ºçš„å›ç­”ã€‚å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦èªªæ˜ã€‚å›ç­”è¦æ¸…æ¥šã€å…·é«”ï¼Œä¸¦ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"""

        try:
            # å‘¼å« Groq API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è¾²æ¥­é¡§å•ï¼Œæ“…é•·å›ç­”å„ç¨®è¾²æ¥­ç›¸é—œå•é¡Œã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=Config.MAX_TOKENS,
                temperature=Config.TEMPERATURE
            )
            
            answer = response.choices[0].message.content.strip()
            return answer
        
        except Exception as e:
            print(f"LLM ç”ŸæˆéŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•ç”Ÿæˆå›ç­”ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„å•ç­”æµç¨‹
        
        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            
        Returns:
            åŒ…å«å›ç­”å’Œä¾†æºçš„å­—å…¸
        """
        # 1. å¾å‘é‡è³‡æ–™åº«æª¢ç´¢ç›¸é—œæ–‡ä»¶
        print(f"ğŸ” æœå°‹ç›¸é—œè³‡æ–™: {question}")
        relevant_docs = self.vector_store.search(question)
        
        if not relevant_docs:
            return {
                'answer': "æŠ±æ­‰ï¼Œæˆ‘åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚è«‹å˜—è©¦æ›å€‹æ–¹å¼æå•ã€‚",
                'sources': []
            }
        
        # 2. ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        print(f"ğŸ¤– ç”Ÿæˆå›ç­”...")
        answer = self.generate_answer(question, relevant_docs)
        
        # 3. æ•´ç†ä¾†æºè³‡è¨Š
        sources = []
        for doc in relevant_docs:
            source_info = {
                'source': doc['metadata'].get('source', 'unknown'),
                'snippet': doc['content'][:100] + '...' if len(doc['content']) > 100 else doc['content']
            }
            sources.append(source_info)
        
        return {
            'answer': answer,
            'sources': sources
        }
    
    def answer_question_simple(self, question: str) -> str:
        """
        ç°¡åŒ–ç‰ˆå•ç­”ï¼ˆç›´æ¥è¿”å›ç­”æ¡ˆæ–‡å­—ï¼‰
        
        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            
        Returns:
            å›ç­”æ–‡å­—
        """
        result = self.answer_question(question)
        return result['answer']
