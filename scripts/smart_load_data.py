"""
æ™ºèƒ½è³‡æ–™è¼‰å…¥è…³æœ¬ - åªè™•ç†æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶
"""
import sys
import os
import json
import hashlib
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.document_loader import DocumentLoader
from src.vector_store import VectorStore
from config import Config


# æª”æ¡ˆè¿½è¹¤è¨˜éŒ„è·¯å¾‘
TRACKING_FILE = os.path.join(Config.VECTOR_DB_PATH, 'file_tracking.json')


def get_file_hash(file_path):
    """è¨ˆç®—æª”æ¡ˆçš„ MD5 hash"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•è¨ˆç®— hash: {file_path} - {e}")
        return None


def load_tracking_data():
    """è¼‰å…¥æª”æ¡ˆè¿½è¹¤è¨˜éŒ„"""
    if os.path.exists(TRACKING_FILE):
        try:
            with open(TRACKING_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}


def save_tracking_data(data):
    """å„²å­˜æª”æ¡ˆè¿½è¹¤è¨˜éŒ„"""
    os.makedirs(os.path.dirname(TRACKING_FILE), exist_ok=True)
    with open(TRACKING_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_files_to_process(data_path):
    """
    å–å¾—éœ€è¦è™•ç†çš„æª”æ¡ˆ
    
    Returns:
        tuple: (æ–°æª”æ¡ˆåˆ—è¡¨, ä¿®æ”¹æª”æ¡ˆåˆ—è¡¨, æœªè®Šæ›´æª”æ¡ˆåˆ—è¡¨)
    """
    tracking_data = load_tracking_data()
    
    new_files = []
    modified_files = []
    unchanged_files = []
    
    # æƒæè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æª”æ¡ˆ
    for root, dirs, files in os.walk(data_path):
        for file in files:
            # åªè™•ç†æ”¯æ´çš„æª”æ¡ˆæ ¼å¼
            if not file.lower().endswith(('.txt', '.pdf', '.docx', '.xlsx')):
                continue
            
            file_path = os.path.join(root, file)
            relative_path = os.path.relpath(file_path, data_path)
            
            # è¨ˆç®—æª”æ¡ˆ hash
            current_hash = get_file_hash(file_path)
            if current_hash is None:
                continue
            
            # å–å¾—æª”æ¡ˆä¿®æ”¹æ™‚é–“
            mtime = os.path.getmtime(file_path)
            
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨æ–¼è¿½è¹¤è¨˜éŒ„ä¸­
            if relative_path in tracking_data:
                old_hash = tracking_data[relative_path].get('hash')
                if old_hash == current_hash:
                    unchanged_files.append(file_path)
                else:
                    modified_files.append(file_path)
            else:
                new_files.append(file_path)
    
    return new_files, modified_files, unchanged_files


def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("è¾²æ¥­çŸ¥è­˜åº« - æ™ºèƒ½è³‡æ–™è¼‰å…¥ç¨‹å¼")
    print("=" * 60)
    
    # æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
    if not os.path.exists(Config.DATA_PATH):
        os.makedirs(Config.DATA_PATH, exist_ok=True)
        print(f"\nå·²å»ºç«‹è³‡æ–™å¤¾: {Config.DATA_PATH}")
        print(f"è«‹å°‡è¾²æ¥­ç›¸é—œæ–‡ä»¶æ”¾å…¥æ­¤è³‡æ–™å¤¾ï¼Œç„¶å¾Œé‡æ–°åŸ·è¡Œæ­¤è…³æœ¬ã€‚")
        return
    
    # åˆ†æéœ€è¦è™•ç†çš„æª”æ¡ˆ
    print(f"\nğŸ“‚ æƒæè³‡æ–™å¤¾: {Config.DATA_PATH}")
    new_files, modified_files, unchanged_files = get_files_to_process(Config.DATA_PATH)
    
    # é¡¯ç¤ºæƒæçµæœ
    print(f"\nğŸ“Š æƒæçµæœ:")
    print(f"   âœ… æœªè®Šæ›´æª”æ¡ˆ: {len(unchanged_files)} å€‹")
    print(f"   ğŸ†• æ–°å¢æª”æ¡ˆ: {len(new_files)} å€‹")
    print(f"   ğŸ“ ä¿®æ”¹æª”æ¡ˆ: {len(modified_files)} å€‹")
    
    # é¡¯ç¤ºè©³ç´°è³‡è¨Š
    if unchanged_files:
        print(f"\nâœ… æœªè®Šæ›´æª”æ¡ˆï¼ˆè·³éï¼‰:")
        for f in unchanged_files:
            print(f"   â€¢ {os.path.basename(f)}")
    
    if new_files:
        print(f"\nğŸ†• æ–°å¢æª”æ¡ˆ:")
        for f in new_files:
            print(f"   â€¢ {os.path.basename(f)}")
    
    if modified_files:
        print(f"\nğŸ“ ä¿®æ”¹æª”æ¡ˆ:")
        for f in modified_files:
            print(f"   â€¢ {os.path.basename(f)}")
    
    # å¦‚æœæ²’æœ‰éœ€è¦è™•ç†çš„æª”æ¡ˆ
    files_to_process = new_files + modified_files
    if not files_to_process:
        print(f"\nâœ… æ‰€æœ‰æª”æ¡ˆéƒ½å·²æ˜¯æœ€æ–°ç‹€æ…‹ï¼Œç„¡éœ€æ›´æ–°ï¼")
        return
    
    # è©¢å•æ˜¯å¦ç¹¼çºŒ
    print(f"\nç¸½å…±éœ€è¦è™•ç† {len(files_to_process)} å€‹æª”æ¡ˆ")
    response = input("æ˜¯å¦ç¹¼çºŒï¼Ÿ(y/n): ")
    if response.lower() != 'y':
        print("å·²å–æ¶ˆ")
        return
    
    # åˆå§‹åŒ–æ–‡ä»¶è¼‰å…¥å™¨
    loader = DocumentLoader(Config.DATA_PATH)
    
    # åªè¼‰å…¥éœ€è¦è™•ç†çš„æª”æ¡ˆ
    all_docs = []
    tracking_data = load_tracking_data()
    
    for file_path in files_to_process:
        relative_path = os.path.relpath(file_path, Config.DATA_PATH)
        print(f"\nğŸ“„ è™•ç†: {os.path.basename(file_path)}")
        
        try:
            # æ ¹æ“šæª”æ¡ˆé¡å‹é¸æ“‡è¼‰å…¥æ–¹æ³•
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.txt':
                docs = loader.load_txt(file_path)
            elif file_ext == '.pdf':
                docs = loader.load_pdf(file_path)
            elif file_ext in ['.docx', '.doc']:
                docs = loader.load_docx(file_path)
            elif file_ext == '.xlsx':
                docs = loader.load_xlsx(file_path)
            else:
                print(f"   âœ— ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}")
                continue
            
            all_docs.extend(docs)
            
            # æ›´æ–°è¿½è¹¤è¨˜éŒ„
            tracking_data[relative_path] = {
                'hash': get_file_hash(file_path),
                'mtime': os.path.getmtime(file_path),
                'processed_at': datetime.now().isoformat(),
                'chunks': len(docs)
            }
            
            print(f"   âœ“ è¼‰å…¥ {len(docs)} å€‹ç‰‡æ®µ")
        except Exception as e:
            print(f"   âœ— è¼‰å…¥å¤±æ•—: {e}")
    
    if not all_docs:
        print("\nâš ï¸  æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ–‡ä»¶")
        return
    
    # åˆ†å‰²æ–‡ä»¶æˆè¼ƒå°çš„å€å¡Š
    print(f"\nâœ‚ï¸  åˆ†å‰²æ–‡ä»¶æˆå€å¡Š...")
    chunked_docs = loader.chunk_documents(all_docs, chunk_size=500, overlap=50)
    print(f"å…± {len(chunked_docs)} å€‹æ–‡ä»¶å€å¡Š")
    
    # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
    print(f"\nğŸ’¾ åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
    vector_store = VectorStore()
    
    # é¡¯ç¤ºè³‡æ–™åº«è³‡è¨Š
    info = vector_store.get_collection_info()
    print(f"é›†åˆåç¨±: {info['name']}")
    print(f"ç¾æœ‰æ–‡ä»¶æ•¸: {info['count']}")
    
    # å¦‚æœæ˜¯ä¿®æ”¹æª”æ¡ˆï¼Œéœ€è¦å…ˆåˆªé™¤èˆŠè³‡æ–™
    if modified_files:
        print(f"\nğŸ—‘ï¸  æ³¨æ„: ä¿®æ”¹çš„æª”æ¡ˆéœ€è¦å…ˆåˆªé™¤èˆŠè³‡æ–™")
        print(f"   å»ºè­°: ä½¿ç”¨å®Œå…¨é‡å»ºæ¨¡å¼")
        response = input("æ˜¯å¦æ¸…é™¤æ‰€æœ‰è³‡æ–™ä¸¦é‡æ–°è¼‰å…¥ï¼Ÿ(y/n): ")
        if response.lower() == 'y':
            vector_store.delete_collection()
            vector_store = VectorStore()
            print("âœ“ å·²æ¸…é™¤èˆŠè³‡æ–™")
            
            # é‡æ–°è¼‰å…¥æ‰€æœ‰æª”æ¡ˆï¼ˆåŒ…å«æœªè®Šæ›´çš„ï¼‰
            print("\nğŸ“‚ é‡æ–°è¼‰å…¥æ‰€æœ‰æª”æ¡ˆ...")
            all_files = unchanged_files + files_to_process
            all_docs = []
            
            for file_path in all_files:
                file_ext = os.path.splitext(file_path)[1].lower()
                try:
                    if file_ext == '.txt':
                        docs = loader.load_txt(file_path)
                    elif file_ext == '.pdf':
                        docs = loader.load_pdf(file_path)
                    elif file_ext in ['.docx', '.doc']:
                        docs = loader.load_docx(file_path)
                    elif file_ext == '.xlsx':
                        docs = loader.load_xlsx(file_path)
                    else:
                        continue
                    all_docs.extend(docs)
                except Exception as e:
                    print(f"   âœ— {os.path.basename(file_path)}: {e}")
            
            chunked_docs = loader.chunk_documents(all_docs, chunk_size=500, overlap=50)
            print(f"å…± {len(chunked_docs)} å€‹æ–‡ä»¶å€å¡Š")
    
    # æ–°å¢æ–‡ä»¶åˆ°å‘é‡è³‡æ–™åº«
    print(f"\nğŸš€ é–‹å§‹æ–°å¢æ–‡ä»¶åˆ°å‘é‡è³‡æ–™åº«...")
    vector_store.add_documents(chunked_docs)
    
    # å„²å­˜è¿½è¹¤è¨˜éŒ„
    save_tracking_data(tracking_data)
    
    # é¡¯ç¤ºæœ€çµ‚è³‡è¨Š
    final_info = vector_store.get_collection_info()
    print(f"\n{'=' * 60}")
    print(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆï¼")
    print(f"{'=' * 60}")
    print(f"é›†åˆåç¨±: {final_info['name']}")
    print(f"æ–‡ä»¶ç¸½æ•¸: {final_info['count']}")
    print(f"å„²å­˜ä½ç½®: {final_info['persist_directory']}")
    print(f"è¿½è¹¤è¨˜éŒ„: {TRACKING_FILE}")
    
    # æ¸¬è©¦æœå°‹åŠŸèƒ½
    print(f"\n{'=' * 60}")
    print("ğŸ§ª æ¸¬è©¦æœå°‹åŠŸèƒ½")
    print(f"{'=' * 60}")
    vector_store.test_search("æ°´ç¨»ç¨®æ¤æ–¹æ³•")


if __name__ == "__main__":
    main()
